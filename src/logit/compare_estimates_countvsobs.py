#%%
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from clogit import ccp
from clogit import clogit
from clogit import utility
from generate_explanatory_variables import create_x_matrix

# compare_sd_errors.py
# The purpose of this py file is a standardized comparison of standard errors and estimates when using counts data (ie. tabulated) vs observation data. 
# As new features expand I would like to keep track of potential errors that may occur.


# %%
# 2. Read in state and decision space, simulated data, and choice probabilities

# Read in simulated data (generated by generate_logit_data.py)
ccps = np.loadtxt("ccps.txt")
df = pd.read_pickle("sim_data.pkl")
decision_space = np.loadtxt("decision_space.txt")
state_space = np.loadtxt("state_space.txt")
state_idx = np.arange(state_space.shape[0])

# Parse data on observed choices and states
y = df.decision_idx.values
state_data = df.state_idx.values
pd.set_option("display.max_rows", 100)
pd.set_option("display.max_columns", 100)

# create count data
df["counts"] = 1

df_count = (
    df.groupby(["consumer_type", "state_idx", "decision_idx"])
    .size()
    .reset_index(name="counts")
)


# %%
# 3. Create explanatory variables x for conditional logit and estimate the model

w_j_vars = ["age_pol", "new", "d1", "trade", "keep"]
s_i_vars = ["age_pol", "nocar"]
w_j_deg = 2
s_i_deg = 2

# x-array for *all* states and decisions
x_space, name_space = create_x_matrix(
    state_space,
    decision_space,
    state_idx,
    w_j_vars,
    w_j_deg,
    s_i_vars,
    s_i_deg,
)


# %% estimate with observations
use_count_data = False
if use_count_data:
    df = df_count.copy()
    y = df.decision_idx.values
    state_data = df.state_idx.values
    counts = df_count["counts"].values
else:
    counts = df["counts"].values


# x-array for *observed* states and decisions
x, name = create_x_matrix(
    state_space,
    decision_space,
    state_data,
    w_j_vars,
    w_j_deg,
    s_i_vars,
    s_i_deg,
)

# 3.1. Estimate the Logit model
res_obs = clogit(
    y,
    x,
    counts,
    cov_type="Ainv",
    theta0=np.zeros(x.shape[2]),
    deriv=2,
    parnames=name,
    quiet=False,
)



# %% estimate with counts

use_count_data = True
if use_count_data:
    df = df_count.copy()
    y = df.decision_idx.values
    state_data = df.state_idx.values
    counts = df_count["counts"].values
else:
    counts = df["counts"].values

# x-array for *observed* states and decisions
x, name = create_x_matrix(
    state_space,
    decision_space,
    state_data,
    w_j_vars,
    w_j_deg,
    s_i_vars,
    s_i_deg,
)

# 3.1. Estimate the Logit model
res_counts = clogit(
    y,
    x,
    counts,
    cov_type="Ainv",
    theta0=np.zeros(x.shape[2]),
    deriv=2,
    parnames=name,
    quiet=False,
)

# %% make comparison table: 

# Creating index 
indexnames = [x for x in res_obs.parnames] + ['timeit']

# Creating multiindex columns 
levelname = 'obs. data'
colnames=pd.MultiIndex.from_tuples(
        [(levelname, 'thetahat'), (levelname, 'standard error')])

# results pd
data =  np.vstack([
    np.array([res_obs.theta_hat.flatten(), res_obs.se.flatten()]).T,
    np.array([res_obs.time, np.nan])],
)

pd_obs=pd.DataFrame(
    data = data,
    index = indexnames, 
    columns= colnames)



# Creating multiindex columns 
levelname = 'count data'
colnames=pd.MultiIndex.from_tuples(
        [(levelname, 'thetahat'), (levelname, 'standard error')])

data =  np.vstack(
    [np.array([res_counts.theta_hat.flatten(), res_counts.se.flatten()]).T,
    np.array([res_counts.time, np.nan])],
)


# results pd
pd_counts=pd.DataFrame(
    data = data,
    index = indexnames, 
    columns= colnames,
)

# combine results
results = pd_obs.join(pd_counts)
results['se_diff'] = results[('obs. data','standard error')] - results[('count data','standard error')]

# %% print results
print(results)
# %%

